{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Bayes - manual implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "NpqQncn-N_Kn",
        "DD0aKmWCzsUp",
        "rvL9zKUo0b0p",
        "loVhh1586nAX",
        "DCtx0dMP9TkN",
        "nLnx4nBO-_iy",
        "aq1zHQd-Y8NX",
        "Vi331QhFW2XF",
        "FTJo06zzLWX3",
        "75T0PJzaHjXC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes algorithm"
      ],
      "metadata": {
        "id": "NpqQncn-N_Kn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from random import seed\n",
        "from random import randrange\n",
        "from math import pi\n",
        "from math import exp"
      ],
      "metadata": {
        "id": "TbsFwdLF1FD7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Five main steps:\n",
        "\n",
        "1. separate all the rows by class (divide the dataset in more datasets, one for each class);\n",
        "2. summarize dataset, namely compute mean, variance and cardinality of each column of the dataset;\n",
        "3. summarize the dataset by class, namely do the same as 2. but for each dataset created at 1.;\n",
        "4. define the probability density function with which modelling the distribution of each column for each class (strong assumption);\n",
        "5. compute the class probabilities by applying the Bayes theorem (simplified).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qKCJqCliOGwC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main goal is to apply the formula:\n",
        "\n",
        "$p(y_i|x_1,x_2,...,x_n) = p(x_1|y_i)p(x_2|y_i)...p(x_n|y_i)p(y_i)$\n",
        "\n",
        "which is the Bayes theorem simplified with two assumptions:\n",
        "1. the variables($x_j$) are independent;\n",
        "2. since the denominator, under 1., becomes a constant ($p(x_1)p(x_2)...p(x_n)$), we can remove it, since we care only about the maximum value, and not about the probability itself."
      ],
      "metadata": {
        "id": "YId1-8AkRIF7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separate by class"
      ],
      "metadata": {
        "id": "DD0aKmWCzsUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can create a dictionary object where each key is the class value and each object is a list of all the records with that class value; the structure is:\n",
        "\n",
        "separated = {class_value1: [entry1, entry2,...], class_value2: [entry1, entry2,...],}\n",
        "\n",
        "where class_valuei represents the i-th class value, and the list contains all entries of the dataset with that class value."
      ],
      "metadata": {
        "id": "gf1sjdeZzuom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def separate_by_class(dataset):\n",
        "  \"\"\"\n",
        "  It returns a dictionary whose keys are the class values and whose objects are lists\n",
        "  of entries of the dataset with the proper class value.\n",
        "  \n",
        "  @param: dataset It is a list of lists, where each inner list is a sample.\n",
        "  \"\"\"\n",
        "\n",
        "  separated = dict()\n",
        "\n",
        "  # for each entry of the dataset, add it to the list associated to its class value\n",
        "  for i in range(len(dataset)):\n",
        "    vector = dataset[i]\n",
        "    class_value = vector[-1] # the class is in the last column\n",
        "\n",
        "    # if the class value is not present in the dictionary, add it as a key along with a new list\n",
        "    if (class_value not in separated):\n",
        "      separated[class_value] = list()\n",
        "\n",
        "    # add the entry to the dictionary\n",
        "    separated[class_value].append(vector)\n",
        "\n",
        "  return separated"
      ],
      "metadata": {
        "id": "wJXLf1FyzwWB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize dataset"
      ],
      "metadata": {
        "id": "rvL9zKUo0b0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to compute all the probabilities we need to make a prediction, we need to know mean, standard deviation and cardinality of each column:"
      ],
      "metadata": {
        "id": "yPCOtBzn01d4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mean\n",
        "def mean(numbers):\n",
        "  '''It calculates the mean of a list of numbers.\n",
        "  \n",
        "  @param: numbers It is a list of numbers.'''\n",
        "\n",
        "  return sum(numbers)/float(len(numbers))"
      ],
      "metadata": {
        "id": "n08kBIKQ0gZ6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# standard deviation\n",
        "def stdev(numbers):\n",
        "  '''It calculates the standard deviation of a list of numbers.\n",
        "  \n",
        "  @param: numbers It is a list of numbers.'''\n",
        "\n",
        "  avg = mean(numbers)\n",
        "  variance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
        "  return sqrt(variance)"
      ],
      "metadata": {
        "id": "-PV8MpnA0_pj"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean, stdev and count for each column in a dataset\n",
        "def summarize_dataset(dataset):\n",
        "  \"\"\"\n",
        "  It returns a list of triples. The returned summaries contains one triple for each\n",
        "  column of dataset (except for the last column); each triple contains (mean, std, len).\n",
        "  \n",
        "  @param: dataset It is a list of lists, where each inner list is a sample of the dataset.\n",
        "  \"\"\"\n",
        "  \n",
        "  # through the asterisk and the zip function I can create an iterable of lists\n",
        "  # whose elements are the columns of the dataset\n",
        "  summaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
        "\n",
        "  # summaries is a list; I delete the last element, which are the statistics for the class column\n",
        "  del(summaries[-1])\n",
        "  \n",
        "  return summaries"
      ],
      "metadata": {
        "id": "dSlRKAV71lJx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summarize data by class"
      ],
      "metadata": {
        "id": "loVhh1586nAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We require statistics from our training dataset organized by class:"
      ],
      "metadata": {
        "id": "tuRGZ3vy6olI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_by_class(dataset):\n",
        "  \"\"\"\n",
        "  It returns a dictionary where the keys are the class values and the objects are\n",
        "  lists of triples. So, to each class value (key) we associate a list of triples, where\n",
        "  each triple contains (mean,std,len) of a column. The cardinality of each list is equal\n",
        "  to the number of columns (features xi) of the dataset.\n",
        "\n",
        "  @param: dataset It is a list of lists, where each inner list is a sample.\n",
        "  \"\"\"\n",
        "\n",
        "  # get a dictionary class_value -> entries\n",
        "  separated = separate_by_class(dataset)\n",
        "\n",
        "  summaries = dict()\n",
        "\n",
        "  # for each list of entries of a class value, compute the statistics and store them into summaries\n",
        "  for class_value, rows in separated.items():\n",
        "    summaries[class_value] = summarize_dataset(rows)\n",
        "    \n",
        "  return summaries"
      ],
      "metadata": {
        "id": "7M6Cgd0q7g4U"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability Density Function"
      ],
      "metadata": {
        "id": "DCtx0dMP9TkN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since calculating the probability of observing a certain real-value given the label is difficult, we assume that the distribution is known."
      ],
      "metadata": {
        "id": "GH5pT-av9Wih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_probability(x, mean, stdev):\n",
        "  \"\"\"\n",
        "  It computes the Gaussian probability distribution function for a value x\n",
        "  given the mean and the standard deviation.\n",
        "\n",
        "  @param: mean It is the mean of the distribution.\n",
        "  @param: stdev It is the standard deviation of the distribution.\n",
        "  \"\"\"\n",
        "  \n",
        "  exponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
        "  return (1 / (sqrt(2 * pi) * stdev)) * exponent"
      ],
      "metadata": {
        "id": "axN8ZM_P-NR_"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Class Probabilities"
      ],
      "metadata": {
        "id": "nLnx4nBO-_iy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a certain sample for which we want to compute the most likely label, we compute the value of:\n",
        "\n",
        "$p(y_i|x_1,x_2,...,x_n) = p(x_1|y_i)p(x_2|y_i)...p(x_n|y_i)p(y_i)$\n",
        "\n",
        "for each of the possible class values."
      ],
      "metadata": {
        "id": "VVwiNEwU_BRz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_probabilities(summaries, sample):\n",
        "  \"\"\"\n",
        "  It computes a dictionary whose keys are the class values yi and whose objects are\n",
        "  the values of p(yi|x1,x2,...xn). To do so, it applies the Bayes formula under\n",
        "  the independency assumption of the features.\n",
        "\n",
        "  @param: summaries It is a dictionary whose keys are the class values and whose objects\n",
        "          are lists of triples, where each triple represents the statistics of a column.\n",
        "  @param: sample It is one sample, one row of the original dataset on which we want to\n",
        "          make a prediction.\n",
        "  \"\"\"\n",
        "\n",
        "  # compute the total number of rows in the dataset\n",
        "  total_rows = sum([summaries[class_value][0][2] for class_value in summaries])\n",
        "\n",
        "  probabilities = dict()\n",
        "\n",
        "  # for each class value yi, compute the value of p(yi|x1,x2,...xn) and add it to probabilities.\n",
        "  for class_value, class_summaries in summaries.items():\n",
        "    \n",
        "    # p(yi) = n_samples_of_the_class / total_n_of_samples\n",
        "    probabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
        "\n",
        "    # for each of the columns (feature xi) of the dataset (the last was previously removed)\n",
        "    for i in range(len(class_summaries)):\n",
        "      mean, stdev, count = class_summaries[i]\n",
        "\n",
        "      # the probability p(yi|x1,x2,...,xn)=p(yi)*p(x1|yi)*p(x2|yi)*...*p(xn|yi)\n",
        "      probabilities[class_value] *= calculate_probability(sample[i], mean, stdev)\n",
        "   \n",
        "  # we return a dictionary that contains, for each class value, the probability:\n",
        "  # class_value yi -> p(yi|x1,x2,...xn)\n",
        "  return probabilities"
      ],
      "metadata": {
        "id": "etX9cMI8CiIA"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "aq1zHQd-Y8NX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Select the class with highest probability to make a prediction:"
      ],
      "metadata": {
        "id": "OMauezBgZTCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the class for a given sample\n",
        "def predict(summaries, sample):\n",
        "\t# compute the possibility yi -> p(yi|x1,x2,...xn) for each class yi\n",
        "\tprobabilities = calculate_class_probabilities(summaries, sample)\n",
        "\tbest_label, best_prob = None, -1\n",
        "\n",
        "  # extract the class with maximum value\n",
        "\tfor class_value, probability in probabilities.items():\n",
        "\t\tif best_label is None or probability > best_prob:\n",
        "\t\t\tbest_prob = probability\n",
        "\t\t\tbest_label = class_value\n",
        "\t\t\t\n",
        "\treturn best_label"
      ],
      "metadata": {
        "id": "DxnrHDopHlbE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm"
      ],
      "metadata": {
        "id": "Vi331QhFW2XF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute a prediction for each sample in the test set; the training set is used only at the beginning to learn the distributions of all the $p(y_i)$ and $p(x_j|y_i)$."
      ],
      "metadata": {
        "id": "BmH2E2KqZl8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Algorithm\n",
        "def naive_bayes(training_set, test_set):\n",
        "\n",
        "  # I compute all the statistics needed on the training_set\n",
        "  summarize = summarize_by_class(training_set)\n",
        "\n",
        "  predictions = list()\n",
        "\n",
        "  # for each row in the test_set I compute the prediction and I append it in predictions list\n",
        "  for row in test_set:\n",
        "    output = predict(summarize, row)\n",
        "    predictions.append(output)\n",
        "\n",
        "  # return a list with all the predictions\n",
        "  return(predictions)"
      ],
      "metadata": {
        "id": "wKpdR1EbImXD"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data - Random under sampling"
      ],
      "metadata": {
        "id": "FTJo06zzLWX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two main approaches to randomly resampling an imbalanced dataset are to delete examples from the majority class, called undersampling, and to duplicate examples from the minority class, called oversampling.\n",
        "\n",
        "\n",
        "*   Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n",
        "*   Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model.\n",
        "\n",
        "Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "sPOo-wd4DH7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RUS(ratio, set):\n",
        "  '''It returns a list obtained by merging two lists, one containing the fraud set and the\n",
        "  other containing the non-fraud set.\n",
        "  The ratio parameter defines the ratio fraud/non-fraud.\n",
        "  e.g.: ratio=1 and len(fraud)= 100 implies that len(new_not_fraud)=100.\n",
        "  \n",
        "  @param: ratio It is the ratio between fraud and non-fraud data; it is defined as: ratio:=fraud_set/non-fraud_set.\n",
        "  @param set It is the set to split.\n",
        "  '''\n",
        "\n",
        "  # initialize the two lists\n",
        "  fraud_set = list()\n",
        "  non_fraud_set = list()\n",
        "\n",
        "  # for each sample in the set, append it to the proper class\n",
        "  for sample in set:\n",
        "    if sample[30] == 1:\n",
        "      fraud_set.append(sample)\n",
        "    else:\n",
        "      non_fraud_set.append(sample)\n",
        "\n",
        "  # compute new length of non_fraud_set\n",
        "  new_length_non_fraud_set = int(len(fraud_set) / ratio)\n",
        "\n",
        "  # shuffle non_fraud_set\n",
        "  np.random.seed(0) # set seed to 0 for reproducibility\n",
        "  np.random.shuffle(non_fraud_set)\n",
        "\n",
        "  # take the first new_length_non_fraud_set elements from non_fraud_set\n",
        "  non_fraud_reduced_set = non_fraud_set[0:new_length_non_fraud_set]\n",
        "\n",
        "  # create a unique list for the new set\n",
        "  new_set = fraud_set\n",
        "  new_set.extend(non_fraud_reduced_set)\n",
        "\n",
        "  # return the whole new set\n",
        "  return new_set"
      ],
      "metadata": {
        "id": "MtklT5zY4bYP"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset and split it into training and test set"
      ],
      "metadata": {
        "id": "G-MMXGHZ7-rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is the definition of the hyperparameters."
      ],
      "metadata": {
        "id": "O10oSmfh9fcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_set / training_set\n",
        "fraction_validation = 0.3 # 30% of test set and 70% of training set\n",
        "\n",
        "# inside the training_set: fraud / non-fraud\n",
        "ratio1 = 1 # 50:50\n",
        "ratio2 = 34/66 # 34:66\n",
        "ratio3 = 1/3 # 25:75\n",
        "\n",
        "# hyperparameter k\n",
        "k = 10"
      ],
      "metadata": {
        "id": "eOq0woFO9AO8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset and divide it:"
      ],
      "metadata": {
        "id": "1p6TDuql-ATa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# normalize it\n",
        "# TODO\n",
        "\n",
        "# parse to numpy and shuffle it\n",
        "dataset = dataset.to_numpy() # not normalized\n",
        "np.random.seed(10)\n",
        "np.random.shuffle(dataset) # shuffle the rows\n",
        "\n",
        "# divide into training_set and test_set\n",
        "num_train = int(dataset.shape[0] * (1 - fraction_validation))\n",
        "training_set = dataset[:num_train,:] # take the first num_train rows\n",
        "test_set = dataset[num_train:,:] # take the last tot-num_train rows\n",
        "\n",
        "# apply random under sampling to the training set\n",
        "training_set1 = RUS(ratio1, training_set)\n",
        "training_set2 = RUS(ratio2, training_set)\n",
        "training_set3 = RUS(ratio3, training_set)\n",
        "\n",
        "# apply random under sampling to the test set\n",
        "test_set1 = RUS(ratio1, test_set)\n",
        "test_set2 = RUS(ratio2, test_set)\n",
        "test_set3 = RUS(ratio3, test_set)"
      ],
      "metadata": {
        "id": "fE90h9Ds70CH"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply the algorithm"
      ],
      "metadata": {
        "id": "75T0PJzaHjXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 1: 50:50\n",
        "predictions1 = naive_bayes(training_set1, test_set1)"
      ],
      "metadata": {
        "id": "OhIUIJH3Blon"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 2: 34:66\n",
        "predictions2 = naive_bayes(training_set2, test_set2)"
      ],
      "metadata": {
        "id": "satHUBWLMDB3"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 3: 25:75\n",
        "predictions3 = naive_bayes(training_set3, test_set3)"
      ],
      "metadata": {
        "id": "iJEzE4jVMEgB"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate algorithm through cross-validation"
      ],
      "metadata": {
        "id": "gxKaMgiwMRwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross validation:**\n",
        "\n",
        "Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice."
      ],
      "metadata": {
        "id": "FXa1u6poP8gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-fold cross-validation:**\n",
        "\n",
        "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. k = 10 is usually used."
      ],
      "metadata": {
        "id": "NdfIHlFFQuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create functions for evaluating the algorithm:"
      ],
      "metadata": {
        "id": "5IIuOn8EMoMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "  '''It returns the n_folds folds.\n",
        "  \n",
        "  @param: dataset It is the dataset to split.\n",
        "  @param: n_folds The number of folds to create.\n",
        "  '''\n",
        "\n",
        "  # the returned value will be a list of lists (folds)\n",
        "  dataset_split = list()\n",
        "\n",
        "  # create a list from the dataset\n",
        "  dataset_copy = dataset.tolist()\n",
        "\n",
        "  # compute the number of elements in each fold\n",
        "  fold_size = int(len(dataset) / n_folds)\n",
        "\n",
        "  # build each fold by popping values from the copy of the dataset\n",
        "  for _ in range(n_folds):\n",
        "  \tfold = list()\n",
        "    # build the fold\n",
        "  \twhile len(fold) < fold_size:\n",
        "  \t\tindex = randrange(len(dataset_copy))\n",
        "  \t\tfold.append(dataset_copy.pop(index))\n",
        "    # append the fold\n",
        "  \tdataset_split.append(fold)\n",
        "   \n",
        "  return dataset_split"
      ],
      "metadata": {
        "id": "BccwKl-8RMej"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\t\"\"\"\n",
        "\tIt computes the accuracy between the two provided lists.\n",
        "\tAccuracy := n_correct / n_actual\n",
        "\n",
        "\t@param: actual It is the list with all the actual values.\n",
        "\t@param: predicted It is the list with all the predicted values.\n",
        "\t\"\"\"\n",
        "\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\t\n",
        "\treturn correct / float(len(actual)) * 100.0"
      ],
      "metadata": {
        "id": "wJg-cs46RLYs"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "  '''It returns a list of accuracies, each of them computed on a certain fold.\n",
        "  \n",
        "  @param: dataset It is the dataset on which evaluating the algorithm.\n",
        "  @param: algorithm It is the algorithm to evaluate; it is a lambda.\n",
        "  @param: n_folds It represents the number of folds on which divide the dataset.\n",
        "  @param: args It contains parameters of the algorithm (k in the case of KNN).\n",
        "  '''\n",
        "\n",
        "  # split the dataset into n_folds\n",
        "  folds = cross_validation_split(dataset, n_folds)\n",
        "\n",
        "  # this list contains the accuracy of the algorithm computed for each fold at a time\n",
        "  scores = list()\n",
        "\n",
        "  # for each fold compute the accuracy\n",
        "  for fold in folds:\n",
        "    training_set = list(folds) # copy the dataset\n",
        "    training_set.remove(fold) # remove the current fold\n",
        "    training_set = sum(training_set, []) # merge all the folds in training_set in one list\n",
        "    test_set = list()\n",
        "\n",
        "    # build the test_set by removing the class in each row of the current fold\n",
        "    for row in fold:\n",
        "      row_copy = list(row)\n",
        "      test_set.append(row_copy)\n",
        "      row_copy[-1] = None\n",
        "\n",
        "    # compute the accuracy and append it to scores\n",
        "    predicted = algorithm(training_set, test_set, *args)\n",
        "    actual = [row[-1] for row in fold] # actual values are in the last column of the fold\n",
        "\n",
        "    # compute the accuracy\n",
        "    accuracy = accuracy_metric(actual, predicted)    \n",
        "    scores.append(accuracy)                 \n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "m2OlnrmqRJkY"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the algorithm:"
      ],
      "metadata": {
        "id": "lCIqeD2ZPrWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_folds = 3\n",
        "rus_dataset = RUS(0.5,dataset)\n",
        "rus_dataset = np.asarray(rus_dataset)\n",
        "\n",
        "# I pass the result of the RUS function on the dataset\n",
        "scores = evaluate_algorithm(rus_dataset, naive_bayes, n_folds)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq1AagVCPsy5",
        "outputId": "675e1906-3884-4001-9ab3-959d448e0a0d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scores: [93.08943089430895, 92.6829268292683, 93.4959349593496]\n",
            "Mean Accuracy: 93.089%\n"
          ]
        }
      ]
    }
  ]
}