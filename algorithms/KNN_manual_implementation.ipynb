{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN - manual implementation.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "aKvPVCxu2PTn",
        "U3fiY9jV-hYf",
        "-zf1uUYSIn93",
        "Y9LkSghrIoEL",
        "G-MMXGHZ7-rc",
        "FE4RX-9fLeMv"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KNN (K-nearest neighbours) algorithm"
      ],
      "metadata": {
        "id": "aKvPVCxu2PTn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "from math import sqrt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n",
        "import jax.numpy as jnp\n",
        "import jax\n",
        "from random import seed\n",
        "from random import randrange"
      ],
      "metadata": {
        "id": "50adVLs8LmtJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Three main steps:\n",
        "1. calculate Euclidean distance;\n",
        "2. get nearest neighbours;\n",
        "3. make predictions."
      ],
      "metadata": {
        "id": "LYlp3XmBFULo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define the distance"
      ],
      "metadata": {
        "id": "U3fiY9jV-hYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are various kinds of distances, for instance Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. For the euclidean distance, I simply compute the L2 norm of two vectors (the rows of the dataset):"
      ],
      "metadata": {
        "id": "8w2YPiboHi6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_distance(row1, row2):\n",
        "  ''' It computes the Euclidean distance between vectors row1 and row2.\n",
        "\n",
        "  @param: row1 First vector.\n",
        "  @param: row2 Second vector.  \n",
        "  '''\n",
        "  \n",
        "  distance = 0.0\n",
        "  for i in range(len(row1)-1): # go up to len(row)-1 because the last element of the row is the output\n",
        "    distance += (row1[i] - row2[i])**2\n",
        "  return sqrt(distance)"
      ],
      "metadata": {
        "id": "wzsXSmuwFRZD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get nearest neighbours"
      ],
      "metadata": {
        "id": "-zf1uUYSIn93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The steps are:\n",
        "1.   take a sample;\n",
        "2.   compute the distance between such sample and every other sample in the dataset;\n",
        "3. sort the samples (in descending order) according to the computed distances;\n",
        "4. select the best k samples (the nearest neighbours).\n",
        "\n"
      ],
      "metadata": {
        "id": "jrDA6vgZI8xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_neighbors(train_set, sample, k):\n",
        "  \"\"\" It returns a list containing the k nearest neighbours of sample in train.\n",
        "\n",
        "  @param: train_set The training set from which extracting the neighbours.\n",
        "  @param: sample The sample to consider; we want its neighbours.\n",
        "  @param: k The number of nearest neighbours to extract.\n",
        "  \"\"\"\n",
        "\n",
        "  distances = list() # here I store all the distances\n",
        "\n",
        "  # compute the distances\n",
        "  for train_row in train_set:\n",
        "    dist = euclidean_distance(sample, train_row)\n",
        "    distances.append((train_row, dist)) # in distances I append pairs of values row-distance\n",
        "\n",
        "  # sort the samples:\n",
        "  # key is a lambda which is executed prior to make comparisons; so we are sorting\n",
        "  # the samples by the second (0-indexed) elements of objects in distances (namely, the distances)\n",
        "  distances.sort(key=lambda tup: tup[1]) \n",
        "\n",
        "  # get the neighbours\n",
        "  neighbors = list()\n",
        "  for i in range(k):\n",
        "    neighbors.append(distances[i][0]) # append only the sample (not the distance)\n",
        "\n",
        "  return neighbors"
      ],
      "metadata": {
        "id": "UeVGBXboIx5L"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make predictions"
      ],
      "metadata": {
        "id": "Y9LkSghrIoEL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a classification task: we just have to compute the most frequent class in the neighborood to carry out the prediction:"
      ],
      "metadata": {
        "id": "R93vA7TWN4to"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_classification(training_set, sample, k):\n",
        "  '''It provides a prediction for sample using the training_set and the best k neighbours.\n",
        "\n",
        "  @param: training_set It is the training set to use for providing a prediction for sample.\n",
        "  @param: sample The sample for which providing the prediction.\n",
        "  @param: k The hyperparameter of the algorithm; it represents the number of nearest neighbours to use.\n",
        "  '''\n",
        "\n",
        "  # get the k-nearest neighbours\n",
        "  neighbors = get_neighbors(training_set, sample, k)\n",
        "\n",
        "  # get the classes of the k-nearest neighbours\n",
        "  output_values = [row[-1] for row in neighbors] # I take row[-1] because the class is the last entry of the row\n",
        "\n",
        "  # perform the prediction: I take the most frequent class (output value)\n",
        "  prediction = max(set(output_values), key=output_values.count)\n",
        "  \n",
        "  return prediction"
      ],
      "metadata": {
        "id": "nAIrA-DNN3M_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithm"
      ],
      "metadata": {
        "id": "80pNJznZGMqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KNN Algorithm\n",
        "def k_nearest_neighbors(training_set, test_set, k):\n",
        "\t\"\"\"\n",
        "\tIt applies the KNN algorithm to each entry of the test_set using the training_set as training set\n",
        "\tand using k as hyperparameter.\n",
        "\n",
        "\t@param: training_set It is the training set to use.\n",
        "\t@param: test_set It is the test set; this function provides a prediction for each entry of the test_set.\n",
        "\t@param: k It represents the main hyperparameter of the algorithm, namely it says how many nearest neighbours\n",
        "\tto use for computing the prediction.\n",
        "\t\"\"\"\n",
        "\n",
        "\t# we return a list of predictions\n",
        "\tpredictions = list()\n",
        " \n",
        "\t# compute a prediction for each entry of the test_set\n",
        "\tfor row in test_set:\n",
        "\t\toutput = predict_classification(training_set, row, k)\n",
        "\t\tpredictions.append(output)\n",
        "\t\n",
        "\treturn(predictions)"
      ],
      "metadata": {
        "id": "ThFdCsp3bFah"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data - Random under sampling"
      ],
      "metadata": {
        "id": "FTJo06zzLWX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two main approaches to randomly resampling an imbalanced dataset are to delete examples from the majority class, called undersampling, and to duplicate examples from the minority class, called oversampling.\n",
        "\n",
        "\n",
        "*   Random oversampling duplicates examples from the minority class in the training dataset and can result in overfitting for some models.\n",
        "*   Random undersampling deletes examples from the majority class and can result in losing information invaluable to a model.\n",
        "\n",
        "Random oversampling involves randomly selecting examples from the minority class, with replacement, and adding them to the training dataset. Random undersampling involves randomly selecting examples from the majority class and deleting them from the training dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "sPOo-wd4DH7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def RUS(ratio, set):\n",
        "  '''It returns a list obtained by merging two lists, one containing the fraud set and the\n",
        "  other containing the non-fraud set.\n",
        "  The ratio parameter defines the ratio fraud/non-fraud.\n",
        "  e.g.: ratio=1 and len(fraud)= 100 implies that len(new_not_fraud)=100.\n",
        "  \n",
        "  @param: ratio It is the ratio between fraud and non-fraud data; it is defined as: ratio:=fraud_set/non-fraud_set.\n",
        "  @param set It is the set to split.\n",
        "  '''\n",
        "\n",
        "  # initialize the two lists\n",
        "  fraud_set = list()\n",
        "  non_fraud_set = list()\n",
        "\n",
        "  # for each sample in the set, append it to the proper class\n",
        "  for sample in set:\n",
        "    if sample[30] == 1:\n",
        "      fraud_set.append(sample)\n",
        "    else:\n",
        "      non_fraud_set.append(sample)\n",
        "\n",
        "  # compute new length of non_fraud_set\n",
        "  new_length_non_fraud_set = int(len(fraud_set) / ratio)\n",
        "  print('new_length: ', new_length_non_fraud_set)\n",
        "\n",
        "  # shuffle non_fraud_set\n",
        "  np.random.seed(0) # set seed to 0 for reproducibility\n",
        "  np.random.shuffle(non_fraud_set)\n",
        "\n",
        "  # take the first new_length_non_fraud_set elements from non_fraud_set\n",
        "  non_fraud_reduced_set = non_fraud_set[0:new_length_non_fraud_set]\n",
        "\n",
        "  # create a unique list for the new set\n",
        "  new_set = fraud_set\n",
        "  new_set.extend(non_fraud_reduced_set)\n",
        "\n",
        "  # return the whole new set\n",
        "  return new_set"
      ],
      "metadata": {
        "id": "MtklT5zY4bYP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the dataset and split it into training and test set"
      ],
      "metadata": {
        "id": "G-MMXGHZ7-rc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First step is the definition of the hyperparameters."
      ],
      "metadata": {
        "id": "O10oSmfh9fcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# test_set / training_set\n",
        "fraction_validation = 0.3 # 30% of test set and 70% of training set\n",
        "\n",
        "# inside the training_set: fraud / non-fraud\n",
        "ratio1 = 1 # 50:50\n",
        "ratio2 = 34/66 # 34:66\n",
        "ratio3 = 1/3 # 25:75\n",
        "\n",
        "# hyperparameter k\n",
        "k = 10"
      ],
      "metadata": {
        "id": "eOq0woFO9AO8"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the dataset and divide it:"
      ],
      "metadata": {
        "id": "1p6TDuql-ATa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "dataset = pd.read_csv('creditcard.csv')\n",
        "\n",
        "# normalize it\n",
        "# TODO\n",
        "\n",
        "# parse to numpy and shuffle it\n",
        "dataset = dataset.to_numpy() # not normalized\n",
        "np.random.seed(10)\n",
        "np.random.shuffle(dataset) # shuffle the rows\n",
        "\n",
        "# divide into training_set and test_set\n",
        "num_train = int(dataset.shape[0] * (1 - fraction_validation))\n",
        "training_set = dataset[:num_train,:] # take the first num_train rows\n",
        "test_set = dataset[num_train:,:] # take the last tot-num_train rows\n",
        "\n",
        "# apply random under sampling to the training set\n",
        "training_set1 = RUS(ratio1, training_set)\n",
        "training_set2 = RUS(ratio2, training_set)\n",
        "training_set3 = RUS(ratio3, training_set)\n",
        "\n",
        "# apply random under sampling to the test set\n",
        "test_set1 = RUS(ratio1, test_set)\n",
        "test_set2 = RUS(ratio2, test_set)\n",
        "test_set3 = RUS(ratio3, test_set)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fE90h9Ds70CH",
        "outputId": "18019a05-ba23-43a4-dfe0-8b4c4f9c3b53"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_length:  173\n",
            "new_length:  335\n",
            "new_length:  519\n",
            "new_length:  71\n",
            "new_length:  137\n",
            "new_length:  213\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Apply the algorithm"
      ],
      "metadata": {
        "id": "FE4RX-9fLeMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 1: 50:50\n",
        "predictions1 = k_nearest_neighbors(training_set1, test_set1, k)"
      ],
      "metadata": {
        "id": "OhIUIJH3Blon"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 2: 34:66\n",
        "predictions2 = k_nearest_neighbors(training_set2, test_set2, k)"
      ],
      "metadata": {
        "id": "satHUBWLMDB3"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# proportion 3: 25:75\n",
        "predictions3 = k_nearest_neighbors(training_set3, test_set3, k)"
      ],
      "metadata": {
        "id": "iJEzE4jVMEgB"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate algorithm through cross-validation"
      ],
      "metadata": {
        "id": "gxKaMgiwMRwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross validation:**\n",
        "\n",
        "Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice."
      ],
      "metadata": {
        "id": "FXa1u6poP8gf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**k-fold cross-validation:**\n",
        "\n",
        "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. k = 10 is usually used."
      ],
      "metadata": {
        "id": "NdfIHlFFQuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create functions for evaluating the algorithm:"
      ],
      "metadata": {
        "id": "5IIuOn8EMoMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, n_folds):\n",
        "  '''It returns the n_folds folds.\n",
        "  \n",
        "  @param: dataset It is the dataset to split.\n",
        "  @param: n_folds The number of folds to create.\n",
        "  '''\n",
        "\n",
        "  # the returned value will be a list of lists (folds)\n",
        "  dataset_split = list()\n",
        "\n",
        "  # create a list from the dataset\n",
        "  dataset_copy = dataset.tolist()\n",
        "\n",
        "  # compute the number of elements in each fold\n",
        "  fold_size = int(len(dataset) / n_folds)\n",
        "\n",
        "  # build each fold by popping values from the copy of the dataset\n",
        "  for _ in range(n_folds):\n",
        "  \tfold = list()\n",
        "    # build the fold\n",
        "  \twhile len(fold) < fold_size:\n",
        "  \t\tindex = randrange(len(dataset_copy))\n",
        "  \t\tfold.append(dataset_copy.pop(index))\n",
        "    # append the fold\n",
        "  \tdataset_split.append(fold)\n",
        "   \n",
        "  return dataset_split"
      ],
      "metadata": {
        "id": "BccwKl-8RMej"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy percentage\n",
        "def accuracy_metric(actual, predicted):\n",
        "\t\"\"\"\n",
        "\tIt computes the accuracy between the two provided lists.\n",
        "\tAccuracy := n_correct / n_actual\n",
        "\n",
        "\t@param: actual It is the list with all the actual values.\n",
        "\t@param: predicted It is the list with all the predicted values.\n",
        "\t\"\"\"\n",
        "\n",
        "\tcorrect = 0\n",
        "\tfor i in range(len(actual)):\n",
        "\t\tif actual[i] == predicted[i]:\n",
        "\t\t\tcorrect += 1\n",
        "\t\n",
        "\treturn correct / float(len(actual)) * 100.0"
      ],
      "metadata": {
        "id": "wJg-cs46RLYs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate an algorithm using a cross validation split\n",
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
        "  '''It returns a list of accuracies, each of them computed on a certain fold.\n",
        "  \n",
        "  @param: dataset It is the dataset on which evaluating the algorithm.\n",
        "  @param: algorithm It is the algorithm to evaluate; it is a lambda.\n",
        "  @param: n_folds It represents the number of folds on which divide the dataset.\n",
        "  @param: args It contains parameters of the algorithm (k in the case of KNN).\n",
        "  '''\n",
        "\n",
        "  # split the dataset into n_folds\n",
        "  folds = cross_validation_split(dataset, n_folds)\n",
        "\n",
        "  # this list contains the accuracy of the algorithm computed for each fold at a time\n",
        "  scores = list()\n",
        "\n",
        "  # for each fold compute the accuracy\n",
        "  for fold in folds:\n",
        "    training_set = list(folds) # copy the dataset\n",
        "    training_set.remove(fold) # remove the current fold\n",
        "    training_set = sum(training_set, []) # merge all the folds in training_set in one list\n",
        "    test_set = list()\n",
        "\n",
        "    # build the test_set by removing the class in each row of the current fold\n",
        "    for row in fold:\n",
        "      row_copy = list(row)\n",
        "      test_set.append(row_copy)\n",
        "      row_copy[-1] = None\n",
        "\n",
        "    # compute the accuracy and append it to scores\n",
        "    predicted = algorithm(training_set, test_set, *args)\n",
        "    actual = [row[-1] for row in fold] # actual values are in the last column of the fold\n",
        "\n",
        "    # compute the accuracy\n",
        "    accuracy = accuracy_metric(actual, predicted)    \n",
        "    scores.append(accuracy)                 \n",
        "\n",
        "  return scores"
      ],
      "metadata": {
        "id": "m2OlnrmqRJkY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the algorithm:"
      ],
      "metadata": {
        "id": "lCIqeD2ZPrWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_folds = 3\n",
        "rus_dataset = RUS(0.5,dataset)\n",
        "rus_dataset = np.asarray(rus_dataset)\n",
        "\n",
        "# I pass the result of the RUS function on the dataset\n",
        "scores = evaluate_algorithm(rus_dataset, k_nearest_neighbors, n_folds, k)\n",
        "print('Scores: %s' % scores)\n",
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq1AagVCPsy5",
        "outputId": "3fffa0c0-0037-41e2-eae7-df738945de02"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "new_length:  488\n",
            "Scores: [69.67213114754098, 75.81967213114754, 73.36065573770492]\n",
            "Mean Accuracy: 72.951%\n"
          ]
        }
      ]
    }
  ]
}